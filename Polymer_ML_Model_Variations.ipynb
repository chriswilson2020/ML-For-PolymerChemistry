{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657acfc8-17d8-43b0-8411-81c1bccddefa",
   "metadata": {},
   "source": [
    "# Polymer Properties Model Refinement\n",
    "Data and methodology taken from: Estimation and Prediction of the Polymersâ€™ Physical Characteristics Using the Machine Learning Models Polymers 2024, 16(1), 115; https://doi.org/10.3390/polym16010115.\n",
    "\n",
    "Github repository: https://github.com/catauggie/polymersML/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c222e6-6773-45f9-b72d-b0a2c05ec922",
   "metadata": {},
   "source": [
    "The goal of this notebook is to begin the refine the model of linear regression using different ML techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59301c8b-554f-44dd-b57a-6fbe03308a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Go ahead and import the data into a dataframe and then make sure its imported properly by listing the first few lines\n",
    "import pandas as pd\n",
    "new_df = pd.read_excel('Tg_Data_Frame.xlsx')\n",
    "new_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d30d82-0537-4446-852f-9257924b09cf",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27fbad-a464-4fca-bdbf-48505d786535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score, median_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_performance(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model using various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_test: array-like of shape (n_samples,) or (n_samples, n_outputs), \n",
    "              True values for X.\n",
    "    - y_pred: array-like of shape (n_samples,) or (n_samples, n_outputs),\n",
    "              Estimated target values.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    variance_score = explained_variance_score(y_test, y_pred)\n",
    "    medae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error: {mse:.3f}')\n",
    "    print(f'R-squared: {r2:.3f}')\n",
    "    print(f'Mean Absolute Error: {mae:.3f}')\n",
    "    print(f'Explained Variance Score: {variance_score:.3f}')\n",
    "    print(f'Median Absolute Error: {medae:.3f}')\n",
    "\n",
    "# Example usage:\n",
    "# y_test = [actual values]\n",
    "# y_pred = [predicted values]\n",
    "# evaluate_model_performance(y_test, y_pred)\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Plot the actual vs. predicted values to evaluate a model's performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_test: array-like, True values.\n",
    "    - y_pred: array-like, Predicted values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))  # Set the figure size for better readability\n",
    "\n",
    "    # Scatter plot for actual vs. predicted values\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, label='Predicted vs. Actual')\n",
    "\n",
    "    # Ideal line for perfect predictions\n",
    "    max_val = max(max(y_test), max(y_pred))  # Find the maximum value for setting plot limits\n",
    "    min_val = min(min(y_test), min(y_pred))  # Find the minimum value for setting plot limits\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Ideal Fit')\n",
    "\n",
    "    # Customization and labels\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Model Predictions vs. Actual Data')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# y_test = [actual values]\n",
    "# y_pred = [predicted values]\n",
    "# plot_actual_vs_predicted(y_test, y_pred)\n",
    "\n",
    "def bland_altman_plot(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Generate a Bland-Altman plot to assess the agreement between two sets of measurements.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_test: array-like, true values.\n",
    "    - y_pred: array-like, predicted values.\n",
    "    \"\"\"\n",
    "    avg = (y_test + y_pred) / 2\n",
    "    diff = y_test - y_pred\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(avg, diff, alpha=0.5)\n",
    "    plt.axhline(y=np.mean(diff), color='r', linestyle='--', label='Mean Difference')\n",
    "    plt.axhline(y=np.mean(diff) + 1.96 * np.std(diff), color='g', linestyle='--', label='Upper Limit of Agreement')\n",
    "    plt.axhline(y=np.mean(diff) - 1.96 * np.std(diff), color='g', linestyle='--', label='Lower Limit of Agreement')\n",
    "    plt.xlabel('Average of Actual and Predicted Values')\n",
    "    plt.ylabel('Difference Between Actual and Predicted Values')\n",
    "    plt.title('Bland-Altman Plot')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# y_test = [actual values]\n",
    "# y_pred = [predicted values]\n",
    "# bland_altman_plot(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e28a24-d8a9-49da-9bce-5d293170b1af",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "The RandomForestRegressor is a popular machine learning algorithm that belongs to the ensemble learning family, specifically within the Random Forests methodology. It operates by constructing a multitude of decision trees at training time and outputting the mean or average prediction of the individual trees. This method is particularly used for regression tasks, where the goal is to predict a continuous outcome variable. \n",
    "\n",
    "Below, I'll outline the pros and cons of using a RandomForestRegressor, its applications, and its potential use in predicting glass transition temperatures from molecular fingerprints.\n",
    "\n",
    "### Pros of RandomForestRegressor\n",
    "* **Accuracy**: RandomForestRegressor is known for providing high accuracy in many prediction tasks due to its ensemble approach, which reduces overfitting by averaging the results of numerous trees.\n",
    "* **Robustness**: It can handle outliers and nonlinear data effectively, making it robust across various datasets.\n",
    "* **Feature Importance**: It inherently provides insights into feature importance, which can be valuable for understanding the factors influencing the prediction.\n",
    "* **Versatility**: Can be used for both regression and classification tasks, making it applicable to a wide range of problems.\n",
    "* **Handling Missing Values**: Capable of handling missing values in the dataset without requiring extensive pre-processing.\n",
    "* **Parallelizable**: The algorithm can be easily parallelized across multiple CPUs for faster processing, which is beneficial for dealing with large datasets.\n",
    "\n",
    "### Cons of RandomForestRegressor\n",
    "* **Complexity**: It creates numerous trees (which can be computationally intensive) and requires more memory and processing power, especially as the number of trees increases.\n",
    "* **Interpretability**: While individual decision trees are interpretable, the ensemble nature of RandomForest makes it more challenging to interpret the model's predictions directly.\n",
    "* **Long Training Time**: For large datasets or a large number of trees, the training time can be significantly longer compared to simpler models.\n",
    "* **Overfitting with Noisy Data**: Despite its robustness to overfitting, in cases of extremely noisy data, the model can still overfit.\n",
    "\n",
    "### Applications\n",
    "RandomForestRegressor is widely applicable in various domains, including:\n",
    "\n",
    "* **Financial Market Analysis**: For predicting stock prices, market trends, and risk assessment.\n",
    "* **Healthcare**: In predicting disease outbreak, patient prognosis, and treatment effectiveness.\n",
    "* **Environmental Modeling**: For forecasting weather patterns, air quality, and climate change effects.\n",
    "* **Real Estate**: In estimating property values based on numerous features.\n",
    "Retail: For demand forecasting and inventory management.\n",
    "\n",
    "### Predicting Glass Transition Temperatures\n",
    "RandomForestRegressor can be particularly suited for this task due to its ability to handle complex, non-linear relationships between features (in this case, molecular fingerprints) and the target variable (glass transition temperatures). The model can capture the intricate patterns and dependencies in the molecular structure that influence the glass transition temperature.\n",
    "\n",
    "The molecular fingerprints, which represent the presence or absence of certain molecular structures and properties, serve as input features for the RandomForestRegressor. By training on these features along with known glass transition temperatures, the model can learn the relationship between the molecular structure and the transition temperature, thereby enabling it to predict the glass transition temperatures of new compounds.\n",
    "\n",
    "This application leverages the RandomForestRegressor's strengths in handling complex, high-dimensional data and its robustness to variations in the data, making it a promising approach for predicting properties of materials based on their molecular composition. However, success in this specific application would also depend on the quality and representativeness of the training data, the selection of relevant features from the molecular fingerprints, and the tuning of the model's hyperparameters to optimize performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d97c0-0333-4014-b8f5-8df65ce950a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Assuming 'df' is your DataFrame\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89844d18-d4ed-4cab-b28b-ef24eb504188",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor\n",
    "Another powerful machine learning algorithm that uses the boosting strategy to improve prediction accuracy. It builds an ensemble of weak prediction models, typically decision trees, in a sequential manner where each tree tries to correct the errors made by the previous ones. This approach focuses on converting weak learners into strong ones over iterations, optimizing a loss function. Below are the pros and cons of using GradientBoostingRegressor, its applications, and its potential use in predicting glass transition temperatures from molecular fingerprints.\n",
    "\n",
    "## Pros of GradientBoostingRegressor\n",
    "* **High Accuracy*: Gradient boosting is capable of producing highly accurate models by systematically addressing errors of previous models through optimization.\n",
    "* **Flexibility**: It can handle various types of data (numerical, categorical) and is adaptable to different loss functions, making it suitable for a wide range of regression and classification tasks.\n",
    "* **Handling Non-linear Data**: Due to its sequential approach in correcting errors, it can model complex non-linear relationships effectively.\n",
    "* **Feature Importance**: Similar to RandomForest, gradient boosting provides insights into which features are most important for making predictions.\n",
    "* **Overfitting Control**: Offers several hyperparameters (like the number of trees, depth of trees, learning rate) that can be fine-tuned to prevent overfitting.\n",
    "\n",
    "## Cons of GradientBoostingRegressor\n",
    "* **Training Time**: The sequential nature of boosting means it can be slower to train compared to models that allow parallelization, such as random forests.\n",
    "* **Complexity and Tuning**: Requires careful tuning of hyperparameters to achieve the best performance without overfitting. This process can be time-consuming and complex.\n",
    "* **Memory Usage**: Can consume more memory than simpler models, especially as the number of trees and depth increases.\n",
    "* **Risk of Overfitting**: If not properly tuned, especially with too many trees or too deep trees, it can overfit the training data.\n",
    "\n",
    "\n",
    "## Applications\n",
    "Gradient Boosting Regressor finds its application in diverse fields, including but not limited to:\n",
    "\n",
    "* **Finance*: For credit scoring, risk management, and algorithmic trading strategies.\n",
    "* **Healthcare**: In disease prediction, personalized medicine, and healthcare resource optimization.\n",
    "* **Energy**: Forecasting electricity demand, renewable energy output predictions, and price forecasting.\n",
    "* **Retail and E-commerce**: For customer lifetime value prediction, sales forecasting, and inventory management.\n",
    "* **Real Estate**: Predicting house prices based on various features like location, size, and amenities.\n",
    "\n",
    "## Predicting Glass Transition Temperatures\n",
    "The prediction of glass transition temperatures from molecular fingerprints is a nuanced task that can benefit from the high accuracy and flexibility of the Gradient Boosting Regressor. This method is particularly useful for capturing the complex, non-linear relationships between the molecular structure (encoded as fingerprints) and their physical properties, such as the glass transition temperature.\n",
    "\n",
    "Molecular fingerprints, which encode the presence of particular molecular features, serve as the input to the model. The Gradient Boosting Regressor can iteratively learn from the subtle nuances in how these features correlate with the glass transition temperatures. By focusing on minimizing the prediction error in each step, it can effectively predict the glass transition temperatures for new, unseen molecules.\n",
    "\n",
    "This application benefits from the model's ability to handle complex datasets and its robustness against overfitting (when properly tuned), making it a potent tool in the field of materials science and chemistry. The success of GradientBoostingRegressor in predicting glass transition temperatures will largely depend on the quality of the molecular fingerprint data, the representativeness of the training dataset, and the optimization of model parameters to the specific characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00331a6-8c59-4171-9a09-fc8d2fffb00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor model\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643f1e5-0b27-4f6b-a395-f0a859811959",
   "metadata": {},
   "source": [
    "Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765ba31-4902-4fe0-9a0c-3bde0298a836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Regressor model\n",
    "model = SVR(kernel='linear')  # You can experiment with different kernels: 'linear', 'poly', 'rbf', etc.\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f34ccd-20d3-45e9-9e97-dadd9d82aa14",
   "metadata": {},
   "source": [
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74502826-791e-4b53-b987-f438ba8da4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Lasso Regression model\n",
    "model = Lasso(alpha=0.1)  # 'alpha' is the regularization strength, you can adjust it based on your data\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9792c0c-000e-4829-8c20-caf3449fc940",
   "metadata": {},
   "source": [
    "Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbead8-ff3c-4132-976d-4c7566c3e0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Elastic Net model\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "# 'alpha' is the regularization strength, and 'l1_ratio' controls the mix between L1 and L2 regularization.\n",
    "# You can adjust these parameters based on your data.\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d329596-73d9-4ae1-9bcb-3c67be8d4a70",
   "metadata": {},
   "source": [
    "K Nebours Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e87f7-a364-427f-9ef7-9f363074f99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the KNN Regressor model\n",
    "model = KNeighborsRegressor(n_neighbors=5)  # 'n_neighbors' is the number of neighbors to consider, you can adjust it\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e6b7d-4df8-413f-9793-c3b5b4ac84e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalized_mse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    var_y = np.var(y_true)\n",
    "    nmse = mse / var_y\n",
    "    return nmse\n",
    "\n",
    "def mean_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    non_zero_indices = y_true != 0\n",
    "    y_true_non_zero = y_true[non_zero_indices]\n",
    "    y_pred_non_zero = y_pred[non_zero_indices]\n",
    "    \n",
    "    # Calculate MPE\n",
    "    mpe = abs(np.mean((y_true_non_zero - y_pred_non_zero) / y_true_non_zero) * 100)\n",
    "    return mpe\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "rmse_result = root_mean_squared_error(y_test, y_pred)\n",
    "nmse_result = normalized_mse(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mpe_result = mean_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse_result}')\n",
    "print(f'Mean Percentage Error: {mpe_result}')\n",
    "print(f'Normalized Mean Squared Error: {nmse_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19ecb4-8ae7-4692-9327-386255556f33",
   "metadata": {},
   "source": [
    "Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c705c-bdbf-4838-9297-ff2ddb4f9e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor(max_depth=5)  # 'max_depth' controls the maximum depth of the tree, you can adjust it\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa357a-a685-4db8-895f-e78f3e4ee178",
   "metadata": {},
   "source": [
    "Decision Tree & Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a9b8c-9d10-4b97-9e34-754cbe7f2d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the base Decision Tree Regressor model\n",
    "base_model = DecisionTreeRegressor(max_depth=5)  # You can adjust 'max_depth' based on your data\n",
    "\n",
    "# Initialize the Bagging Regressor model\n",
    "model = BaggingRegressor(base_model, n_estimators=10, random_state=42)\n",
    "# 'n_estimators' is the number of base models (Decision Trees) in the ensemble, you can adjust it\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbc1c3-3552-4e3f-8739-aa039b36c6c0",
   "metadata": {},
   "source": [
    "Ada Boost and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bebcef-d112-4462-9cd9-f5f76460e36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the base Decision Tree Regressor model\n",
    "base_model = DecisionTreeRegressor(max_depth=5)  # You can adjust 'max_depth' based on your data\n",
    "\n",
    "# Initialize the AdaBoost Regressor model\n",
    "model = AdaBoostRegressor(base_model, n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "# 'n_estimators' is the number of base models (Decision Trees) in the ensemble, and 'learning_rate' scales the contribution of each model\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dfaf0-9d4e-494d-a198-85f28ee71a78",
   "metadata": {
    "tags": []
   },
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36425e-9daa-4218-962a-62cb57d0dbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# X should be your features, and y should be your target variable\n",
    "cols = [c for c in new_df.columns if 'col' in c]\n",
    "X = new_df[cols]\n",
    "y = new_df['Glass transition temperature_value_median']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor model\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "# 'n_estimators' is the number of boosting rounds, 'learning_rate' scales the contribution of each tree, and 'max_depth' controls the maximum depth of each tree\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "evaluate_model_performance(y_test, y_pred)\n",
    "plot_actual_vs_predicted(y_test, y_pred)\n",
    "bland_altman_plot(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
