{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4cf77c-6e9a-415d-bdec-09d4f08e0f1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Polymer Properties Neural Networks\n",
    "\n",
    "**At this point we continue to diverge from the original publciation thus we acknolwege only the dataset**\n",
    "Data taken from: Estimation and Prediction of the Polymersâ€™ Physical Characteristics Using the Machine Learning Models Polymers 2024, 16(1), 115; https://doi.org/10.3390/polym16010115.\n",
    "\n",
    "Github repository: https://github.com/catauggie/polymersML/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ded43-c711-492d-a765-d6ecbc3602e6",
   "metadata": {},
   "source": [
    "## Moving on from traditional ML into NN\n",
    "The models and techniques described in the previous sections are are not neural networks. Instead, they are examples of traditional machine learning models and techniques used for regression tasks. Here's a brief overview of what was discussed:\n",
    "\n",
    "1. **Linear Regression**: A simple linear approach to modeling the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "2. **Stacking Regressor**: A model that combines multiple regression models for predictions. The base level models are trained on the full training set, and then a meta-model is trained on the outputs of the base models as features.\n",
    "3. **Pipeline with Feature Selection and Dimensionality Reduction**: This involves preprocessing steps like selecting the most important features (`SelectKBest`) and reducing the number of dimensions in the data (`PCA`), before applying a final regression model (like `LinearRegression`). This is a method to improve model performance by reducing overfitting or computational complexity.\n",
    "\n",
    "Neural networks are a different class of models inspired by the structure and function of the brain's neural networks. They are typically used for more complex tasks like image and speech recognition, natural language processing, and deep learning applications where they can automatically and adaptively learn spatial hierarchies of features from data.\n",
    "\n",
    "The models mentioned above do not involve neural network architecture; they are part of classical machine learning. Neural networks would involve models like:\n",
    "\n",
    "- **Multilayer Perceptrons (MLP)**: A class of feedforward artificial neural network (ANN).\n",
    "- **Convolutional Neural Networks (CNNs)**: Mostly used in image recognition and processing.\n",
    "- **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory (LSTM)** networks: Primarily used for sequential data such as time series analysis or natural language processing.\n",
    "\n",
    "Each of these neural network types has its own architecture and is designed to handle specific types of data and tasks more effectively than traditional machine learning models in certain scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fad7ea-05bd-497f-ac69-488d6d951044",
   "metadata": {},
   "source": [
    "Whether a neural network would provide better results for predicting glass transition temperatures from molecular fingerprints depends on several factors, including the complexity of the data, the amount of available data, and the specific characteristics of the molecular fingerprints. \n",
    "\n",
    "**Here's an analysis considering these factors**:\n",
    "\n",
    "### Complexity of the Data\n",
    "\n",
    "- **Molecular fingerprints** are typically high-dimensional, sparse datasets that represent the presence or absence of certain molecular substructures or features. Neural networks, especially deep learning models, can be very effective at handling high-dimensional data and automatically discovering intricate patterns that may not be easily captured by traditional machine learning models.\n",
    "- If the relationship between molecular fingerprints and glass transition temperatures is highly nonlinear and complex, neural networks could potentially offer better performance due to their ability to model complex functions.\n",
    "\n",
    "### Amount of Available Data\n",
    "\n",
    "- Neural networks, and deep learning models in particular, usually require large amounts of data to train effectively without overfitting. If a substantial dataset is available, a neural network could be a strong candidate.\n",
    "- For smaller datasets, traditional machine learning models might perform better, or techniques like data augmentation or transfer learning could be considered to make neural networks viable.\n",
    "\n",
    "### Specific Characteristics of Molecular Fingerprints\n",
    "\n",
    "- Neural networks can automatically learn and derive features during the training process, which could be advantageous if the molecular fingerprints require complex feature engineering to reveal underlying patterns related to glass transition temperatures.\n",
    "- Convolutional Neural Networks (CNNs) or Graph Neural Networks (GNNs) could be particularly useful if the molecular fingerprints can be represented as images or graphs, respectively, leveraging the spatial structure of molecules for prediction.\n",
    "\n",
    "### Computational Resources and Model Interpretability\n",
    "\n",
    "- Training neural networks typically requires more computational resources and time than traditional models. This is an important consideration, especially for large datasets or complex models.\n",
    "- Neural networks are often considered \"black box\" models, meaning their predictions can be difficult to interpret compared to simpler, traditional models. In scientific fields where understanding the relationship between variables is important, this could be a drawback.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "A neural network could potentially offer better results for predicting glass transition temperatures from molecular fingerprints if the dataset is large and complex enough to warrant its use. However, it's essential to weigh the benefits against the potential drawbacks, such as the need for more data, computational resources, and the complexity of model interpretation.\n",
    "\n",
    "A practical approach would be to start with simpler models to establish a baseline performance and then experiment with more complex neural network models to see if they provide a significant improvement in prediction accuracy. Cross-validation and comparison using a consistent set of metrics would be essential steps in determining the best model for this specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6c2461-e6cb-41fb-8336-c6eab2feae30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU details:  {'device_name': 'METAL'}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  details = tf.config.experimental.get_device_details(gpus[0])\n",
    "  print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d7bf05-8db7-458e-ae8c-099a83f8f629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 12s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 10:30:03.006991: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-02-23 10:30:03.007014: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-02-23 10:30:03.007019: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-02-23 10:30:03.007072: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-23 10:30:03.007107: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 10:30:06.494041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 70s 77ms/step - loss: 4.7909 - accuracy: 0.0788\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 4.2791 - accuracy: 0.1193\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 3.9043 - accuracy: 0.1577\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 3.9026 - accuracy: 0.1483\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 58s 75ms/step - loss: 3.4581 - accuracy: 0.2106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a7297050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "  include_top=True,\n",
    "  weights=None,\n",
    "  input_shape=(32, 32, 3),\n",
    "  classes=100,)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
